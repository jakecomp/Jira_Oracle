{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re   \n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sklearn.preprocessing\n",
        "import keras_nlp\n",
        "import keras_tuner\n",
        "import sentencepiece\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Method for getting input\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "# from nltk.corpus import stopwords\n",
        "# import nltk\n",
        "# nltk.download(\"punkt\")\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download(\"stopwords\")\n",
        "\n",
        "# def parse_text(text):\n",
        "\n",
        "#     ## Tokenize string into words (and punctuation)\n",
        "#     word_array = word_tokenize(text)\n",
        "#     word_array = [word.lower() for word in word_array if word.isalpha()]\n",
        "\n",
        "#     ## Filter out stop words\n",
        "#     stop_words = set(stopwords.words(\"english\"))\n",
        "#     filtered_words = [word for word in word_array if word.casefold() not in stop_words]\n",
        "\n",
        "#     ## Turn words into lemmatized words\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     lemitized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "#     ## Apply Stemming (Find the roots of similar words)\n",
        "#     stemmer = PorterStemmer()\n",
        "#     stemmed_words = [stemmer.stem(word) for word in lemitized_words]\n",
        "\n",
        "    # joined_words = \"\"\n",
        "    # #for word in stemmed_words:\n",
        "    # joined_words = \" \".join(list(stemmed_words))\n",
        "    # return joined_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###makes clean file\n",
        "\n",
        "# import csv\n",
        "# csv.field_size_limit(10000000)\n",
        "# text_list = []\n",
        "# with open(\"master_dataset.csv\", newline='') as f:\n",
        "#     #reader = csv.reader(f)\n",
        "#     data = list(csv.reader(f))\n",
        "# #print(data[:10])\n",
        "# for row in data:\n",
        "    \n",
        "#     #print(parse_text(row[2]))\n",
        "#     text_list.append(str(parse_text(row[2])))\n",
        "\n",
        "# #print(text_list[:10])\n",
        "# with open(\"master_dataset_clean.csv\", 'w') as f2:\n",
        "#     #writer = csv.writer(f2)\n",
        "#     for x in text_list:\n",
        "#         f2.write(x)\n",
        "#         f2.write('\\n')\n",
        "        #writer.writerows([row])\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tickets = pd.read_csv(\"master_dataset_clean.csv\", skip_blank_lines = False, keep_default_na=False)\n",
        "tickets = tickets.fillna('[UNK]')\n",
        "tickets2 = pd.read_csv(\"labelled_dataset.csv\")\n",
        "tickets3 = pd.DataFrame()\n",
        "\n",
        "#print(tickets['desc'][:3])\n",
        "#tickets['desc'] = [parse_text(text) for text in tickets['desc']]\n",
        "#tickets['desc'] = tickets['desc'].apply(parse_text())\n",
        "#print(tickets['desc'][:3])\n",
        "\n",
        "text_column = tickets['desc']\n",
        "tickets3 = pd.concat([tickets3,text_column], axis = 1)\n",
        "label_column = tickets2['Grade'].astype(int)\n",
        "label_column = label_column - 1\n",
        "tickets3 = pd.concat([tickets3,label_column], axis = 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_column, label_column, test_size=0.2, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True)\n",
        "\n",
        "y_train_cat = to_categorical(y_train, 3)\n",
        "y_test_cat = to_categorical(y_test, 3)\n",
        "y_val_cat = to_categorical(y_val, 3)\n",
        "\n",
        "\n",
        "X_train_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "X_test_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "X_val_dataset = tf.data.Dataset.from_tensor_slices(X_val)\n",
        "# y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "# y_test_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train_cat)\n",
        "y_test_dataset = tf.data.Dataset.from_tensor_slices(y_test_cat)\n",
        "y_val_dataset = tf.data.Dataset.from_tensor_slices(y_val_cat)\n",
        "\n",
        "test_dataset = tf.data.Dataset.zip((X_test_dataset, y_test_dataset))\n",
        "train_dataset = tf.data.Dataset.zip((X_train_dataset, y_train_dataset))\n",
        "val_dataset = tf.data.Dataset.zip((X_val_dataset, y_val_dataset))\n",
        "\n",
        "# print(train_dataset.element_spec)\n",
        "# print(test_dataset.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX2G_saMwWt-",
        "outputId": "ceaf8303-1037-4027-ef93-cf06a9ba64d9"
      },
      "outputs": [],
      "source": [
        "np.array(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd4_BGKyurao",
        "outputId": "3106e4b1-66e8-4b85-df7b-8b4ef39c47af"
      },
      "outputs": [],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000 #10000\n",
        "BATCH_SIZE = 128 #64\n",
        "VOCAB_SIZE = 2500 #1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqkvdcFv41wC",
        "outputId": "5208d200-8f62-46eb-b038-6f2dd7124dca"
      },
      "outputs": [],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 2500 #1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBoyjjWg0Ac9",
        "outputId": "a56ca642-7a69-4820-b723-74d3220a2683"
      },
      "outputs": [],
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGc7C9WiwRWs",
        "outputId": "09b2b8a6-8eb2-48ba-b080-a9e6b0ca03fd"
      },
      "outputs": [],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_tD0QY5wXaK",
        "outputId": "816a4547-4c2e-4097-8c94-0f03180ad3a0"
      },
      "outputs": [],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model.\n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output.\n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "# model = tf.keras.Sequential([\n",
        "#     encoder,\n",
        "#     tf.keras.layers.Embedding(\n",
        "#         input_dim=len(encoder.get_vocabulary()),\n",
        "#         output_dim=64,\n",
        "#         # Use masking to handle the variable sequence lengths\n",
        "#         mask_zero=True),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "#     tf.keras.layers.Dense(64, activation='relu'),\n",
        "#     #tf.keras.layers.Dense(1)\n",
        "#     tf.keras.layers.Dense(3, activation='softmax')\n",
        "# ])\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    #keras_nlp.layers.FNetEncoder(intermediate_dim=64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a8-CwfKebw",
        "outputId": "ac262fcd-bc91-4e22-80ad-0dd4e0de5b51"
      },
      "outputs": [],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O41gw3KfWHus",
        "outputId": "e26e436a-617a-4368-9722-a0e176b86793"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIgpuTeFNDzq",
        "outputId": "943c7c80-0979-438c-a9ce-9ec972577b80"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    #optimizer=tf.keras.optimizers.SGD(),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    metrics=[\"accuracy\"] #tf.keras.metrics.CategoricalAccuracy()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#find weights for labels\n",
        "\n",
        "samp0 = 54046\n",
        "samp1 = 24077\n",
        "samp2 = 9863\n",
        "tot_samples = samp0 + samp1 + samp2\n",
        "\n",
        "w0 = tot_samples / (3*samp0)\n",
        "w1 = tot_samples / (3*samp1)\n",
        "w2 = tot_samples / (3*samp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw86wWS4YgR2",
        "outputId": "cd7c3836-389f-4fee-8ed4-9203c5bd034c"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, \n",
        "                    epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30,\n",
        "                    #class_weight = {0:w0,1:w1,2:w2},\n",
        "                    #use_multiprocessing = True,\n",
        "                    #workers = 5\n",
        "                    )\n",
        "#class weight has made it worse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaNbXi43YgUT",
        "outputId": "8433bfa2-58eb-4c92-d8e7-48f88cf185e3"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "OZmwt_mzaQJk",
        "outputId": "d2c660e9-dff7-4bc7-bda7-370d8574992c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXgfQSgRW6zU",
        "outputId": "818068a0-0550-4ba3-d4d7-4f362fee68e6"
      },
      "outputs": [],
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)\n",
        "\n",
        "print(np.argsmax(predictions[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset1 = train_dataset.shuffle(BUFFER_SIZE).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset1 = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset1 = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 2500 #1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset1.map(lambda text, label: text))\n",
        "\n",
        "\n",
        "# train_dataset2 = train_dataset.shuffle(512).prefetch(16).cache()\n",
        "# test_dataset2 = test_dataset.shuffle(512).prefetch(16).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model2 = tf.keras.Sequential([\n",
        "#     encoder,\n",
        "#     tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 128, mask_zero=True),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "#     keras_nlp.layers.FNetEncoder(intermediate_dim=128),\n",
        "#     keras_nlp.layers.FNetEncoder(intermediate_dim=128),\n",
        "#     keras_nlp.layers.FNetEncoder(intermediate_dim=128),\n",
        "#     tf.keras.layers.Dropout(0.1),\n",
        "#     tf.keras.layers.Dense(3, activation='softmax')\n",
        "# ])\n",
        "\n",
        "model2 = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 8, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4)),\n",
        "    keras_nlp.layers.FNetEncoder(intermediate_dim=8),\n",
        "    keras_nlp.layers.FNetEncoder(intermediate_dim=8),\n",
        "    keras_nlp.layers.FNetEncoder(intermediate_dim=8),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "# input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "# x = encoder((input_ids))\n",
        "# # x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "# #     vocabulary_size=VOCAB_SIZE,\n",
        "# #     sequence_length=512,\n",
        "# #     embedding_dim=128,\n",
        "# #     mask_zero=True,\n",
        "# # )(input_ids)\n",
        "\n",
        "# x = keras_nlp.layers.FNetEncoder(intermediate_dim=128)(inputs=x)\n",
        "# x = keras_nlp.layers.FNetEncoder(intermediate_dim=128)(inputs=x)\n",
        "# x = keras_nlp.layers.FNetEncoder(intermediate_dim=128)(inputs=x)\n",
        "\n",
        "\n",
        "# x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "# x = keras.layers.Dropout(0.1)(x)\n",
        "# outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "# fnet_classifier = keras.Model(input_ids, outputs, name=\"fnet_classifier\")\n",
        "\n",
        "\n",
        "\n",
        "model2.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    #optimizer=tf.keras.optimizers.SGD(),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    metrics=[\"accuracy\"] #tf.keras.metrics.CategoricalAccuracy()\n",
        ")\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data_size = len(X_test)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(0.1 * num_train_steps)\n",
        "initial_learning_rate=2e-5\n",
        "\n",
        "print(steps_per_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history2 = model2.fit(train_dataset1, \n",
        "                    epochs=100,\n",
        "                    validation_data=val_dataset1,\n",
        "                    validation_steps=30,\n",
        "                    steps_per_epoch=steps_per_epoch\n",
        "                    #class_weight = {0:w0,1:w1,2:w2},\n",
        "                    #use_multiprocessing = True,\n",
        "                    #workers = 5\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = model2.evaluate(test_dataset1)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.evaluate(test_dataset1, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history2, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history2, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up epochs and steps\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data_size = len(X_test)\n",
        "steps_per_epoch = int(train_data_size / BATCH_SIZE)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(0.1 * num_train_steps)\n",
        "initial_learning_rate=2e-5\n",
        "\n",
        "print(steps_per_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save model\n",
        "#!mkdir -p saved_model\n",
        "model2.save('saved_model/FNetEncoder_Model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the exact same model, including its weights and the optimizer\n",
        "new_model = tf.keras.models.load_model('saved_model/FNetEncoder_Model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "new_model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use kera_tuner to find optimal hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# [n.name for n in tf.compat.v1.get_default_graph().as_graph_def().node] \n",
        "# tf.compat.v1.reset_default_graph()\n",
        "#tickets = pd.read_csv(\"combined_dataset.csv\", skip_blank_lines = False, keep_default_na=False)\n",
        "tickets = pd.read_csv(\"master_dataset_clean.csv\", skip_blank_lines = False, keep_default_na=False)\n",
        "#tickets = pd.read_csv(\"combined_dataset2.csv\")\n",
        "tickets = tickets.fillna('[UNK]')\n",
        "tickets2 = pd.read_csv(\"labelled_dataset.csv\")\n",
        "tickets2 = pd.read_csv(\"master_dataset.csv\")\n",
        "tickets3 = pd.DataFrame()\n",
        "\n",
        "# print(tickets['desc'][:3])\n",
        "# tickets['desc'] = [parse_text(text) for text in tickets['desc']]\n",
        "# tickets['desc'] = tickets['desc'].apply(parse_text())\n",
        "# print(tickets['desc'][:3])\n",
        "\n",
        "text_column = tickets['desc']\n",
        "tickets3 = pd.concat([tickets3,text_column], axis = 1)\n",
        "label_column = tickets2['effort(s)'].astype(int)\n",
        "#label_column = label_column - 1\n",
        "tickets3 = pd.concat([tickets3,label_column], axis = 1)\n",
        "\n",
        "## simplify data ~ 10000\n",
        "df_low_effort = tickets3[tickets3.Grade == 0].sample(30000)\n",
        "df_med_effort = tickets3[tickets3.Grade == 1].sample(30000, replace = True)\n",
        "df_hig_effort = tickets3[tickets3.Grade == 2].sample(30000, replace = True)\n",
        "\n",
        "#df_effort = tickets3.sample(90000)\n",
        "\n",
        "# print(len(df_low_effort))\n",
        "# print(len(df_med_effort))\n",
        "# print(len(df_hig_effort))\n",
        "\n",
        "result_df = pd.concat([df_low_effort, df_med_effort, df_hig_effort], axis=0)\n",
        "\n",
        "text_column = result_df['desc']\n",
        "label_column = result_df['Grade'].astype(int)\n",
        "# text_column = df_effort['desc']\n",
        "# label_column = df_effort['effort(s)'].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(text_column, label_column, test_size=0.2, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True)\n",
        "\n",
        "# y_train_cat = to_categorical(y_train, 3)\n",
        "# y_test_cat = to_categorical(y_test, 3)\n",
        "# y_val_cat = to_categorical(y_val, 3)\n",
        "\n",
        "# max_train = y_train.max()\n",
        "# min_train = y_train.min()\n",
        "# y_train = (y_train - min_train)/(max_train - min_train)\n",
        "# y_test = (y_test - min_train)/(max_train - min_train)\n",
        "# y_val = (y_val - min_train)/(max_train - min_train)\n",
        "\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat))\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat))\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_cat))\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# test_dataset = tf.data.Dataset.zip((X_test_dataset, y_test_dataset))\n",
        "# train_dataset = tf.data.Dataset.zip((X_train_dataset, y_train_dataset))\n",
        "# val_dataset = tf.data.Dataset.zip((X_val_dataset, y_val_dataset))\n",
        "# train_datasetnoc = tf.data.Dataset.zip((X_train_dataset, y_train_datasetnoc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=string, numpy=b'enabl java portabl flink postcommit test jenkin'>,\n",
              " <tf.Tensor: shape=(), dtype=int64, numpy=1800>)"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.take(1).get_single_element()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_dataset2 = train_dataset.shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "# test_dataset2 = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "# val_dataset2 = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "# VOCAB_SIZE = 2500 #1000\n",
        "# encoder = tf.keras.layers.TextVectorization(\n",
        "#     max_tokens=VOCAB_SIZE)\n",
        "# encoder.adapt(train_dataset2.map(lambda text, label: text))\n",
        "# counts = np.bincount(train_targets[:, 0])\n",
        "samp0 = 54046\n",
        "samp1 = 24077\n",
        "samp2 = 9863\n",
        "tot_samples = samp0 + samp1 + samp2\n",
        "\n",
        "w0 = tot_samples / (3*samp0)\n",
        "w1 = tot_samples / (3*samp1)\n",
        "w2 = tot_samples / (3*samp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "MAX_SEQUENCE_LENGTH = 256#512\n",
        "VOCAB_SIZE = 25993 #25993 15000\n",
        "\n",
        "EMBED_DIM = 128\n",
        "INTERMEDIATE_DIM = 512\n",
        "\n",
        "def train_word_piece(ds, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = ds.map(lambda x, y: x)\n",
        "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(tf.data.AUTOTUNE),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_1150603/2413079574.py\", line 3, in <module>\n",
            "    vocab = train_word_piece(train_dataset, VOCAB_SIZE, reserved_tokens)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_1150603/4070957178.py\", line 11, in train_word_piece\n",
            "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_nlp/src/tokenizers/word_piece_tokenizer_trainer.py\", line 164, in compute_word_piece_vocabulary\n",
            "    vocab = learner.learn(\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow_text/tools/wordpiece_vocab/wordpiece_tokenizer_learner_lib.py\", line 465, in learn\n",
            "    vocab = learn_binary_search(filtered_counts, lower_search, upper_search,\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow_text/tools/wordpiece_vocab/wordpiece_tokenizer_learner_lib.py\", line 364, in learn_binary_search\n",
            "    current_vocab = learn_with_thresh(word_counts, thresh, params)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow_text/tools/wordpiece_vocab/wordpiece_tokenizer_learner_lib.py\", line -1, in learn_with_thresh\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n",
            "    frames.append(self.format_record(record))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n",
            "    frame_info.lines, Colors, self.has_colors, lvals\n",
            "    ^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
            "    return self._sd.lines\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "                                               ^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
            "    pieces = self.included_pieces\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "                                               ^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
            "    pos = scope_pieces.index(self.executing_piece)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "                                               ^^^^^^^^^^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
            "    return only(\n",
            "           ^^^^^\n",
            "  File \"/home/birdy/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n",
            "    raise NotOneValueFound('Expected one value, found 0')\n",
            "executing.executing.NotOneValueFound: Expected one value, found 0\n"
          ]
        }
      ],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\"]\n",
        "#train_sentences = [element[0] for element in train_dataset]\n",
        "vocab = train_word_piece(train_dataset, VOCAB_SIZE, reserved_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(r'word_vocab_stem.txt', 'w') as fp:\n",
        "    for item in vocab:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(r'word_vocab_stem.txt', 'r') as fp:\n",
        "    vocab = fp.read().splitlines()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=False,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence:  tf.Tensor(b'follow todo remov noformat todo add row noformat updat link ticket noformat fixm http fixm http noformat noformat todo cluster initi flow complet todo add busi lock init method http todo need wait futur init noformat', shape=(), dtype=string)\n",
            "Tokens:  tf.Tensor(\n",
            "[  62 1460   84   53 1460   42  171   53   70  152  366   53   82  836\n",
            "   32   82  836   32   53   53 1460  184  212  596  186 1460   42 1950\n",
            "  352  236   47   32 1460   44  298  508  236   53    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0], shape=(256,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b'follow todo remov noformat todo add row noformat updat link ticket noformat fixm http fixm http noformat noformat todo cluster initi flow complet todo add busi lock init method http todo need wait futur init noformat [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "input_sentence_ex = train_dataset.take(1).get_single_element()[0]\n",
        "input_tokens_ex = tokenizer(input_sentence_ex)\n",
        "\n",
        "print(\"Sentence: \", input_sentence_ex)\n",
        "print(\"Tokens: \", input_tokens_ex)\n",
        "print(\"Recovered text after detokenizing: \", tokenizer.detokenize(input_tokens_ex))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "221.82580183210965\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "len_of_lines = 0\n",
        "for x in text_column:\n",
        "    len_of_lines += len(x)\n",
        "    i += 1\n",
        "avg_len_lines = len_of_lines/i    \n",
        "print(avg_len_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "log_dir = \"logs/scalers/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
        "\n",
        "# file_writer = tf.summary.FileWriter(log_dir + \"/metrics\")\n",
        "# file_writer.set_as_default()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5184000"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l = encoder(X_train)\n",
        "ll = encoder2(X_train)\n",
        "max(label_column)\n",
        "#scaler(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  59 1174  157 ...    0    0    0]\n",
            " [1172 1306 1263 ...    0    0    0]\n",
            " [  45  201   61 ...    0    0    0]\n",
            " ...\n",
            " [ 368  628   80 ...    0    0    0]\n",
            " [   1 1532  551 ...    0    0    0]\n",
            " [ 366   35   50 ...    0    0    0]], shape=(10000, 256), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[  57 1172  155 ...    0    0    0]\n",
            " [1170 1304 1261 ...    0    0    0]\n",
            " [  43  199   59 ...    0    0    0]\n",
            " ...\n",
            " [ 366  626   78 ...    0    0    0]\n",
            " [1844 5066 1530 ...    0    0    0]\n",
            " [ 364   33   48 ...    0    0    0]], shape=(10000, 256), dtype=int32)\n",
            "tf.Tensor(\n",
            "[-0.00982499 -0.12772794  0.13453987 -0.02867759 -0.01791551  0.07591633\n",
            " -0.01407235 -0.10305414  0.0303663   0.09495379  0.04088686  0.00633177\n",
            "  0.07807508  0.12348113 -0.04962645  0.06221182 -0.03585226 -0.04159736\n",
            "  0.04247887 -0.04733267 -0.02745271 -0.07568157 -0.02647848 -0.05995679\n",
            " -0.0318312  -0.02928777  0.01596187 -0.08794676 -0.04824246 -0.03926285\n",
            "  0.02734531 -0.01256751 -0.06733508  0.05182472  0.08768432 -0.01539979\n",
            "  0.08926345  0.10699915  0.06237726  0.00789701  0.01824163  0.01624558\n",
            " -0.09293494  0.08592711  0.02189052 -0.02908707 -0.05694509 -0.10097694\n",
            " -0.08416032  0.03624159 -0.10071575  0.1289868  -0.05513766  0.04827603\n",
            " -0.12223855  0.09944163 -0.14274698 -0.02669018 -0.08863133  0.03172929\n",
            "  0.09876849  0.03364164  0.08396848  0.07292715  0.11743119  0.04660822\n",
            "  0.07982931 -0.08104937  0.0912493   0.0289049  -0.11231454  0.07343108\n",
            "  0.03353143 -0.04251559 -0.0087601  -0.0376077   0.08524798 -0.01158874\n",
            " -0.10078347  0.03164625  0.05562866  0.10904722 -0.12267438  0.06454749\n",
            "  0.10784237 -0.02356407  0.07525282 -0.07471503  0.11232571 -0.02299897\n",
            "  0.02377624 -0.06757383 -0.0457028  -0.0099733  -0.03854007  0.01769964\n",
            "  0.10709266  0.00073736  0.05264835 -0.10402334 -0.07560135  0.09693031\n",
            " -0.14496815  0.09924924  0.00928743  0.0314087   0.03515725  0.09597335\n",
            " -0.04733726  0.06326964 -0.04897716 -0.07005312 -0.00372346  0.1310901\n",
            " -0.0272818  -0.01143591 -0.10219371  0.06323184  0.02309226  0.0361089\n",
            "  0.08667829  0.03628234 -0.1145409   0.1071635   0.02055414 -0.09535454\n",
            "  0.02975181  0.04698551], shape=(128,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[-0.00982499 -0.12772794  0.13453987 -0.02867759 -0.01791551  0.07591633\n",
            " -0.01407235 -0.10305414  0.0303663   0.09495379  0.04088686  0.00633177\n",
            "  0.07807508  0.12348113 -0.04962645  0.06221182 -0.03585226 -0.04159736\n",
            "  0.04247887 -0.04733267 -0.02745271 -0.07568157 -0.02647848 -0.05995679\n",
            " -0.0318312  -0.02928777  0.01596187 -0.08794676 -0.04824246 -0.03926285\n",
            "  0.02734531 -0.01256751 -0.06733508  0.05182472  0.08768432 -0.01539979\n",
            "  0.08926345  0.10699915  0.06237726  0.00789701  0.01824163  0.01624558\n",
            " -0.09293494  0.08592711  0.02189052 -0.02908707 -0.05694509 -0.10097694\n",
            " -0.08416032  0.03624159 -0.10071575  0.1289868  -0.05513766  0.04827603\n",
            " -0.12223855  0.09944163 -0.14274698 -0.02669018 -0.08863133  0.03172929\n",
            "  0.09876849  0.03364164  0.08396848  0.07292715  0.11743119  0.04660822\n",
            "  0.07982931 -0.08104937  0.0912493   0.0289049  -0.11231454  0.07343108\n",
            "  0.03353143 -0.04251559 -0.0087601  -0.0376077   0.08524798 -0.01158874\n",
            " -0.10078347  0.03164625  0.05562866  0.10904722 -0.12267438  0.06454749\n",
            "  0.10784237 -0.02356407  0.07525282 -0.07471503  0.11232571 -0.02299897\n",
            "  0.02377624 -0.06757383 -0.0457028  -0.0099733  -0.03854007  0.01769964\n",
            "  0.10709266  0.00073736  0.05264835 -0.10402334 -0.07560135  0.09693031\n",
            " -0.14496815  0.09924924  0.00928743  0.0314087   0.03515725  0.09597335\n",
            " -0.04733726  0.06326964 -0.04897716 -0.07005312 -0.00372346  0.1310901\n",
            " -0.0272818  -0.01143591 -0.10219371  0.06323184  0.02309226  0.0361089\n",
            "  0.08667829  0.03628234 -0.1145409   0.1071635   0.02055414 -0.09535454\n",
            "  0.02975181  0.04698551], shape=(128,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(l)\n",
        "print(ll)\n",
        "embed= keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "            vocabulary_size=len(vocab),\n",
        "            sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "            embedding_dim=EMBED_DIM,\n",
        "            mask_zero=True,\n",
        "        )\n",
        "outputz = embed(ll)\n",
        "print(outputz[3][1])\n",
        "#keras_nlp.layers.PositionEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "%rm -rf ./saved_model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    vocabulary = vocab,\n",
        "    output_sequence_length = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "encoder2 = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=False,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        ")\n",
        "\n",
        "def r2_score(y_test, y_pred):\n",
        "    SS_res =  tf.keras.backend.sum(tf.keras.backend.square( y_test-y_pred )) \n",
        "    SS_tot = tf.keras.backend.sum(keras.backend.square( y_test - keras.backend.mean(y_test) ) ) \n",
        "    return ( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
        "# label_mean = y_train.mean()\n",
        "# train_std = y_train.std()\n",
        "\n",
        "# scaler = tf.keras.layers.Rescaling(scale=1./max(label_column), \n",
        "#                                     offset=0.0\n",
        "#                                     )\n",
        "# scaler = tf.keras.layers.Rescaling(scale=(label - label_mean)/train_std, \n",
        "#                                     offset=0.0\n",
        "#                                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#scaler = sklearn.preprocessing.MinMaxScaler()\n",
        "# scaler.fit_transform(y_train)\n",
        "# scaler.transform(y_test)\n",
        "# scaler.transform(y_val)\n",
        "# scaler = StandardScaler()\n",
        "# train_features = scaler.fit_transform(train_features)\n",
        "\n",
        "# val_features = scaler.transform(val_features)\n",
        "# test_features = scaler.transform(test_features)\n",
        "\n",
        "\n",
        "#train_dataset.as_numpy_iterator\n",
        "\n",
        "def format_dataset(sentence, label):\n",
        "    #label = (label - min(label))/(max(label) - min(label))#(X - min(X)) / (max(X) - min(X))\n",
        "    sentence = encoder2(sentence)\n",
        "    #label = scaler.transform((label))\n",
        "    #return ({\"input_ids\": sentence}, label)\n",
        "    return (sentence, label)\n",
        "\n",
        "\n",
        "def make_dataset(dataset):\n",
        "    #dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.cache().shuffle(1000).batch(64).map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE) #dataset.batch(64).shuffle(256).prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "#norm with only train values\n",
        "# label_mean = y_train.np.mean()\n",
        "# train_std = y_train.std()\n",
        "\n",
        "full_train_ds = make_dataset(train_dataset)\n",
        "train_ds = make_dataset(train_dataset.take(10000))\n",
        "val_ds = make_dataset(val_dataset)\n",
        "test_ds = make_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%export` not found.\n"
          ]
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_mean_squared_error', patience=3, restore_best_weights=True)\n",
        "\n",
        "%export TF_ENABLE_AUTO_MIXED_PRECISION=1\n",
        "%export TF_XLA_FLAGS=--tf_xla_auto_jit=1\n",
        "%export TF_GPU_ALLOCATOR=cuda_malloc_async"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from saved_model/classif_hypertuning/tuner0.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from saved_model/classif_hypertuning/tuner0.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def hyper_class(hp):\n",
        "    model = keras.Sequential([\n",
        "        # keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\"),\n",
        "        # keras.Input(shape=(None,), dtype=\"int64\"),\n",
        "        # keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        #     vocabulary_size=VOCAB_SIZE,\n",
        "        #     sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        #     embedding_dim=EMBED_DIM,\n",
        "        #     mask_zero=True,\n",
        "        # ),\n",
        "        #keras.Input(shape=(None,), dtype=\"int64\"),\n",
        "        #encoder2,\n",
        "        keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "            vocabulary_size=len(vocab),#VOCAB_SIZE,\n",
        "            sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "            embedding_dim=64,\n",
        "            mask_zero=True,\n",
        "            dtype=\"float32\"\n",
        "        ),\n",
        "        # tf.keras.layers.Embedding(len(encoder2.get_vocabulary()),\n",
        "        #                         #input_length = MAX_SEQUENCE_LENGTH,\n",
        "        #                         output_dim=hp.Int(\"output_dim\", min_value=8, max_value=128, step=32), \n",
        "        #                         mask_zero=True),\n",
        "        #tf.keras.layers.Flatten(),\n",
        "        #tf.keras.layers.Reshape((None,MAX_SEQUENCE_LENGTH,128), input_shape=(MAX_SEQUENCE_LENGTH,128)),\n",
        "\n",
        "        #tf.keras.layers.Reshape((MAX_SEQUENCE_LENGTH,128), input_shape=(None,None,MAX_SEQUENCE_LENGTH,128)),\n",
        "        #tf.reshape(X, shape=[tf.shape(X)[0]*tf.shape(x)[1],2]),\n",
        "        #keras.layers.GlobalAveragePooling2D(),\n",
        "        #tf.keras.layers.Dropout(rate=hp.Float(\"rate\", 0.1, 0.99, sampling=\"log\")),\n",
        "        # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=hp.Int(\"units\", min_value=32, max_value=512, step=32))),\n",
        "        # tf.keras.layers.Dropout(rate=hp.Float(\"rate\", 0.1, 0.8, sampling=\"log\")),\n",
        "        keras_nlp.layers.FNetEncoder(intermediate_dim=hp.Int(\"intermediate_dim1\", min_value=32, max_value=512, step=32),\n",
        "                                    dropout=hp.Float(\"dropout1\", 0.1, 0.8, sampling=\"log\")),\n",
        "        keras_nlp.layers.FNetEncoder(intermediate_dim=hp.Int(\"intermediate_dim2\", min_value=32, max_value=512, step=32),\n",
        "                                    dropout=hp.Float(\"dropout2\", 0.1, 0.8, sampling=\"log\")),\n",
        "        keras_nlp.layers.FNetEncoder(intermediate_dim=hp.Int(\"intermediate_dim3\", min_value=32, max_value=512, step=32),\n",
        "                                    dropout=hp.Float(\"dropout3\", 0.1, 0.8, sampling=\"log\")),\n",
        "        #keras.layers.GlobalAveragePooling2D(),\n",
        "        keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dropout(rate=hp.Float(\"rate\", 0.1, 0.8, sampling=\"log\")),\n",
        "        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=hp.Int(\"units\", min_value=4, max_value=128, step=8), return_sequences=hp.Boolean(\"return_sequences\"))),\n",
        "        \n",
        "        # tf.keras.layers.Dense(3, activation='softmax'),\n",
        "        #tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "\n",
        "    ])\n",
        "    \n",
        "    # model.compile(\n",
        "    #     optimizer=tf.keras.optimizers.AdamW(learning_rate=hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
        "    #                                         weight_decay=hp.Float(\"weight_decay\", min_value=1e-4, max_value=1e-2, sampling=\"log\")), \n",
        "    #     loss=\"categorical_crossentropy\", \n",
        "    #     metrics=[keras.metrics.CategoricalAccuracy(), \n",
        "    #             tf.keras.metrics.AUC(name=\"roc\"),\n",
        "    #             tf.keras.metrics.AUC(curve='PR', name='pr'),\n",
        "    #             keras.metrics.CategoricalCrossentropy(),\n",
        "    #             tf.keras.losses.MeanSquaredError(),\n",
        "    #             tf.keras.losses.MeanAbsoluteError(),\n",
        "    #     ],\n",
        "    #     jit_compile=True\n",
        "        \n",
        "    model.compile(\n",
        "        # optimizer=tf.keras.optimizers.AdamW(learning_rate=hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
        "        #                                     weight_decay=hp.Float(\"weight_decay\", min_value=1e-4, max_value=1e-2, sampling=\"log\")), \n",
        "        optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(tf.keras.optimizers.AdamW(\n",
        "                    learning_rate=hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
        "                    weight_decay=hp.Float(\"weight_decay\", min_value=1e-4, max_value=1e-2, sampling=\"log\"))),\n",
        "        loss=\"mean_squared_error\", \n",
        "        metrics=[\n",
        "                #tf.keras.metrics.AUC(name=\"roc\"),\n",
        "                #tf.keras.metrics.AUC(curve='PR', name='pr'),\n",
        "                #keras.metrics.CategoricalCrossentropy(),\n",
        "                r2_score,\n",
        "                tf.keras.metrics.MeanSquaredError(),\n",
        "                tf.keras.metrics.MeanAbsoluteError(),\n",
        "                tf.keras.metrics.MeanAbsolutePercentageError(),\n",
        "                tf.keras.metrics.MeanSquaredLogarithmicError(),\n",
        "                tf.keras.metrics.RootMeanSquaredError(),\n",
        "                #tf.keras.metrics.Accuracy(), \n",
        "                'acc'\n",
        "        ],\n",
        "        jit_compile=True\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "tuner = keras_tuner.Hyperband(\n",
        "    hypermodel=hyper_class,\n",
        "    # Objective is one of the keys.\n",
        "    # Maximize the negative MAE, equivalent to minimize MAE.\n",
        "    objective=[ #keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "                keras_tuner.Objective('val_mean_squared_error', direction=\"min\"),\n",
        "                #eras_tuner.Objective(\"val_roc\", direction=\"max\"),\n",
        "                #keras_tuner.Objective(\"val_pr\", direction=\"max\"),\n",
        "                #keras_tuner.Objective(\"val_categorical_crossentropy\", direction=\"min\"),\n",
        "                ],\n",
        "    max_epochs=30,\n",
        "    directory=\"saved_model\",\n",
        "    project_name=\"classif_hypertuning\",\n",
        "    hyperband_iterations = 2\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from saved_model/classif_hypertuning_nonorm/tuner0.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def hyper_class(hp):\n",
        "    model_unnorm = keras.Sequential([\n",
        "\n",
        "        keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "            vocabulary_size=len(vocab),#VOCAB_SIZE,\n",
        "            sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "            embedding_dim=64,\n",
        "            mask_zero=True,\n",
        "            dtype=\"float32\"\n",
        "        ),\n",
        "\n",
        "        keras_nlp.layers.FNetEncoder(intermediate_dim=hp.Int(\"intermediate_dim1\", min_value=32, max_value=512, step=32),\n",
        "                                    dropout=hp.Float(\"dropout1\", 0.1, 0.8, sampling=\"log\")),\n",
        "        keras_nlp.layers.FNetEncoder(intermediate_dim=hp.Int(\"intermediate_dim2\", min_value=32, max_value=512, step=32),\n",
        "                                    dropout=hp.Float(\"dropout2\", 0.1, 0.8, sampling=\"log\")),\n",
        "        keras_nlp.layers.FNetEncoder(intermediate_dim=hp.Int(\"intermediate_dim3\", min_value=32, max_value=512, step=32),\n",
        "                                    dropout=hp.Float(\"dropout3\", 0.1, 0.8, sampling=\"log\")),\n",
        "        #keras.layers.GlobalAveragePooling2D(),\n",
        "        keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dropout(rate=hp.Float(\"rate\", 0.1, 0.8, sampling=\"log\")),\n",
        "        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=hp.Int(\"units\", min_value=4, max_value=128, step=8), return_sequences=hp.Boolean(\"return_sequences\"))),\n",
        "        \n",
        "        # tf.keras.layers.Dense(3, activation='softmax'),\n",
        "        #tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "        tf.keras.layers.Dense(1),\n",
        "\n",
        "    ])\n",
        "    \n",
        "\n",
        "        \n",
        "    model_unnorm.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(learning_rate=hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
        "                                            weight_decay=hp.Float(\"weight_decay\", min_value=1e-4, max_value=1e-2, sampling=\"log\")), \n",
        "        loss=\"mean_squared_error\", \n",
        "        metrics=[\n",
        "                #tf.keras.metrics.AUC(name=\"roc\"),\n",
        "                #tf.keras.metrics.AUC(curve='PR', name='pr'),\n",
        "                #keras.metrics.CategoricalCrossentropy(),\n",
        "                r2_score,\n",
        "                tf.keras.metrics.MeanSquaredError(),\n",
        "                tf.keras.metrics.MeanAbsoluteError(),\n",
        "                tf.keras.metrics.MeanAbsolutePercentageError(),\n",
        "                tf.keras.metrics.MeanSquaredLogarithmicError(),\n",
        "                tf.keras.metrics.RootMeanSquaredError(),\n",
        "                #tf.keras.metrics.Accuracy(), \n",
        "                'acc'\n",
        "        ],\n",
        "        jit_compile=True\n",
        "    )\n",
        "    return model_unnorm\n",
        "\n",
        "\n",
        "tuner = keras_tuner.Hyperband(\n",
        "    hypermodel=hyper_class,\n",
        "    # Objective is one of the keys.\n",
        "    # Maximize the negative MAE, equivalent to minimize MAE.\n",
        "    objective=[ #keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "                keras_tuner.Objective('val_mean_squared_error', direction=\"min\"),\n",
        "                #eras_tuner.Objective(\"val_roc\", direction=\"max\"),\n",
        "                #keras_tuner.Objective(\"val_pr\", direction=\"max\"),\n",
        "                #keras_tuner.Objective(\"val_categorical_crossentropy\", direction=\"min\"),\n",
        "                ],\n",
        "    max_epochs=30,\n",
        "    directory=\"saved_model\",\n",
        "    project_name=\"classif_hypertuning_nonorm\",\n",
        "    hyperband_iterations = 2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 85 Complete [00h 03m 10s]\n",
            "multi_objective: 3706295808.0\n",
            "\n",
            "Best multi_objective So Far: 1329121408.0\n",
            "Total elapsed time: 01h 39m 34s\n",
            "\n",
            "Search: Running Trial #86\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "160               |320               |intermediate_dim1\n",
            "0.38052           |0.46995           |dropout1\n",
            "96                |224               |intermediate_dim2\n",
            "0.63873           |0.4655            |dropout2\n",
            "480               |448               |intermediate_dim3\n",
            "0.33553           |0.15393           |dropout3\n",
            "0.78165           |0.51724           |rate\n",
            "0.0009476         |0.0067938         |lr\n",
            "0.0071545         |0.00062834        |weight_decay\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "3                 |3                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(train_ds,\n\u001b[1;32m      2\u001b[0m             \u001b[39m# x=x_trn, \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m             \u001b[39m# y=y_trn,\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m             \u001b[39m#class_weight = {0:w0,1:w1,2:w2}, \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m             validation_data\u001b[39m=\u001b[39;49mval_ds, \n\u001b[1;32m      6\u001b[0m             callbacks \u001b[39m=\u001b[39;49m [callback, tensorboard_callback])\n\u001b[1;32m      8\u001b[0m \u001b[39m# tuner.search(x=train_ds_x,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m#             y=train_ds_y, \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#             class_weight = {0:w0,1:w1,2:w2}, \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#             validation_data=(val_ds_x, val_ds_y), \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m#             callbacks = [callback, tensorboard_callback])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_models()[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/engine/base_tuner.py:230\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/engine/base_tuner.py:270\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m    269\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    271\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[1;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/engine/base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 235\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[1;32m    237\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[1;32m    238\u001b[0m     ):\n\u001b[1;32m    239\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    240\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    241\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    243\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    251\u001b[0m         )\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/tuners/hyperband.py:425\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     fit_kwargs[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mvalues[\u001b[39m\"\u001b[39m\u001b[39mtuner/epochs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    424\u001b[0m     fit_kwargs[\u001b[39m\"\u001b[39m\u001b[39minitial_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mvalues[\u001b[39m\"\u001b[39m\u001b[39mtuner/initial_epoch\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 425\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/engine/tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    286\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[0;32m--> 287\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[1;32m    289\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[1;32m    290\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/engine/tuner.py:214\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[1;32m    213\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 214\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mfit(hp, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[1;32m    216\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras_tuner/engine/hypermodel.py:144\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 894\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n\u001b[1;32m    896\u001b[0m   \u001b[39m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn\u001b[39m.\u001b[39m_call_flat(   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    898\u001b[0m       filtered_flat_args,\n\u001b[1;32m    899\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn\u001b[39m.\u001b[39mcaptured_inputs)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:966\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    960\u001b[0m     \u001b[39mlen\u001b[39m(args) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    961\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mmap\u001b[39m(is_tensor_spec, args))\n\u001b[1;32m    962\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mmap\u001b[39m(is_tensor_spec, kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    963\u001b[0m ):\n\u001b[1;32m    964\u001b[0m   \u001b[39m# For the case inputs are not empty and input types are all tf.TensorSpec\u001b[39;00m\n\u001b[1;32m    965\u001b[0m   concrete_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_concrete_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 966\u001b[0m   \u001b[39mreturn\u001b[39;00m compiler_ir\u001b[39m.\u001b[39mfrom_concrete_function(concrete_fn)\n\u001b[1;32m    968\u001b[0m concrete_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_concrete_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    969\u001b[0m fn_name \u001b[39m=\u001b[39m concrete_fn\u001b[39m.\u001b[39mname\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1752\u001b[0m     arg_names\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   1753\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m<arg\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m   1754\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(arg_names), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_positional_args))\n\u001b[1;32m   1755\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(arg_names)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1757\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpretty_printed_signature\u001b[39m(\u001b[39mself\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1758\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a string summarizing the signature of this concrete function.\"\"\"\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m verbose:\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_TapeGradientFunctions\u001b[39;00m(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    379\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Caches forward and backward functions compatible with eager gradients.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[0;32m--> 381\u001b[0m \u001b[39m  In contrast to the delayed-rewrite approach in\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m  `_DelayedRewriteGradientFunctions` which only works with delayed execution,\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m  the forward function generated by this class has a fixed set of outputs which\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39m  may be preserved by a tape in order to compute gradients later.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m  This class is abstract; its child classes differ in how many side outputs of\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m  the forward function their backward function accepts gradients for, which\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m  determines whether higher-order tape gradients are possible.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, func_graph, attrs, func_graph_deleter,\n\u001b[1;32m    392\u001b[0m                forwardprop_input_indices, delayed_rewrite_functions,\n\u001b[1;32m    393\u001b[0m                need_gradients_for_jvps):\n\u001b[1;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph \u001b[39m=\u001b[39m func_graph\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m     53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Bad pipe message: %s [b' 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\\r\\nAccept: text/html,']\n",
            "Bad pipe message: %s [b'plication/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\\r\\nAccept-Language: en-C']\n",
            "Bad pipe message: %s [b'en-US;q=0.7,en;q=0.3\\r\\nAccept-Encoding: gzip, deflate, br\\r\\nConnec']\n"
          ]
        }
      ],
      "source": [
        "tuner.search(train_ds,\n",
        "            # x=x_trn, \n",
        "            # y=y_trn,\n",
        "            #class_weight = {0:w0,1:w1,2:w2}, \n",
        "            validation_data=val_ds, \n",
        "            callbacks = [callback, tensorboard_callback])\n",
        "\n",
        "# tuner.search(x=train_ds_x,\n",
        "#             y=train_ds_y, \n",
        "#             class_weight = {0:w0,1:w1,2:w2}, \n",
        "#             validation_data=(val_ds_x, val_ds_y), \n",
        "#             callbacks = [callback, tensorboard_callback])\n",
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from saved_model/classif_hypertuning/tuner0.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from saved_model/classif_hypertuning/tuner0.json\n"
          ]
        }
      ],
      "source": [
        "tuner = keras_tuner.Hyperband(\n",
        "    hypermodel=hyper_class,\n",
        "    # Objective is one of the keys.\n",
        "    # Maximize the negative MAE, equivalent to minimize MAE.\n",
        "    objective=[ #keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "                keras_tuner.Objective('val_mean_squared_error', direction=\"min\"),\n",
        "                #eras_tuner.Objective(\"val_roc\", direction=\"max\"),\n",
        "                #keras_tuner.Objective(\"val_pr\", direction=\"max\"),\n",
        "                #keras_tuner.Objective(\"val_categorical_crossentropy\", direction=\"min\"),\n",
        "                ],\n",
        "    max_epochs=30,\n",
        "    directory=\"saved_model\",\n",
        "    project_name=\"classif_hypertuning\",\n",
        "    hyperband_iterations = 2\n",
        ")\n",
        "#best_trial = tuner.oracle.get_best_trials(1)\n",
        "#best_trial = tuner.get_best_trials(num_trials=1)\n",
        "#reloaded_model = tuner.get_best_models()[0] #load_model(best_trial)\n",
        "best_hyperparams = tuner.get_best_hyperparameters()[0]\n",
        "top_model = tuner.hypermodel.build(best_hyperparams)\n",
        "#reloaded_model.build()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Weights for model 'sequential' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reloaded_model\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mrnn_models/my_model.keras\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39m# new_model = tf.keras.models.load_model('my_model.keras')\u001b[39;00m\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/engine/training.py:3540\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3529\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3531\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3532\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m   3533\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3538\u001b[0m     \u001b[39m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[1;32m   3539\u001b[0m     \u001b[39m# defined.\u001b[39;00m\n\u001b[0;32m-> 3540\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3541\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m have not yet been \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3542\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWeights are created when the model is first called on \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3545\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Weights for model 'sequential' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`."
          ]
        }
      ],
      "source": [
        "reloaded_model.save('rnn_models/my_model.keras')\n",
        "# new_model = tf.keras.models.load_model('my_model.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Launching TensorBoard..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logs #--bind_all\n",
        "#%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All background processes were killed.\n"
          ]
        }
      ],
      "source": [
        "# %killbgscripts tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# file_writer = tf.summary.FileWriter('/tmp/tb_logs', tf.sess.graph)\n",
        "\n",
        "#best_model.build(input_shape=(None, 28, 28))\n",
        "#best_model.summary()\n",
        "best_hyperparams = tuner.get_best_hyperparameters()[0]\n",
        "top_model = tuner.hypermodel.build(best_hyperparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in saved_model/classif_hypertuning\n",
            "Showing 10 best trials\n",
            "MultiObjective(name=\"multi_objective\", direction=\"min\"): [Objective(name=\"val_mean_squared_error\", direction=\"min\")]\n",
            "\n",
            "Trial 0055 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 512\n",
            "dropout1: 0.11428114568757548\n",
            "intermediate_dim2: 128\n",
            "dropout2: 0.1880763857651768\n",
            "intermediate_dim3: 256\n",
            "dropout3: 0.36738043008209137\n",
            "rate: 0.24686186951876748\n",
            "lr: 0.0002502739699524452\n",
            "weight_decay: 0.007652624915370285\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 4.2013736674562097e-05\n",
            "\n",
            "Trial 0042 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 32\n",
            "dropout1: 0.3009694899001408\n",
            "intermediate_dim2: 64\n",
            "dropout2: 0.14883165525899547\n",
            "intermediate_dim3: 64\n",
            "dropout3: 0.34158324421371616\n",
            "rate: 0.24457145401045055\n",
            "lr: 0.0008444640793833608\n",
            "weight_decay: 0.00632161088105593\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 3\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0024\n",
            "Score: 4.202299169264734e-05\n",
            "\n",
            "Trial 0059 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 224\n",
            "dropout1: 0.10839329725216318\n",
            "intermediate_dim2: 480\n",
            "dropout2: 0.1148700240887886\n",
            "intermediate_dim3: 192\n",
            "dropout3: 0.10132844801887234\n",
            "rate: 0.4603842223944515\n",
            "lr: 0.00041576769288620234\n",
            "weight_decay: 0.006013846979888529\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 4.202458148938604e-05\n",
            "\n",
            "Trial 0005 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 320\n",
            "dropout1: 0.6645581993871603\n",
            "intermediate_dim2: 32\n",
            "dropout2: 0.2876835368812611\n",
            "intermediate_dim3: 288\n",
            "dropout3: 0.26395242515751427\n",
            "rate: 0.13306342080758968\n",
            "lr: 0.0007098923102025065\n",
            "weight_decay: 0.005306279417939182\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 3\n",
            "tuner/round: 0\n",
            "Score: 4.203047865303233e-05\n",
            "\n",
            "Trial 0060 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 384\n",
            "dropout1: 0.41267046175538336\n",
            "intermediate_dim2: 192\n",
            "dropout2: 0.3691227896067878\n",
            "intermediate_dim3: 64\n",
            "dropout3: 0.13755757185228093\n",
            "rate: 0.10446182248559771\n",
            "lr: 0.001703871063991941\n",
            "weight_decay: 0.0017055878173943449\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 4.203510616207495e-05\n",
            "\n",
            "Trial 0052 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 96\n",
            "dropout1: 0.347100722786858\n",
            "intermediate_dim2: 192\n",
            "dropout2: 0.10093064178986816\n",
            "intermediate_dim3: 288\n",
            "dropout3: 0.2863421365405311\n",
            "rate: 0.49808098192620504\n",
            "lr: 0.0004476505694074481\n",
            "weight_decay: 0.0001785607046695224\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 4.205027289572172e-05\n",
            "\n",
            "Trial 0035 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 416\n",
            "dropout1: 0.11752365307834428\n",
            "intermediate_dim2: 416\n",
            "dropout2: 0.5830173680318427\n",
            "intermediate_dim3: 224\n",
            "dropout3: 0.342194244401818\n",
            "rate: 0.13260344811535132\n",
            "lr: 0.0005889065040017682\n",
            "weight_decay: 0.0014318293848778305\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 3\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0027\n",
            "Score: 4.2052415665239096e-05\n",
            "\n",
            "Trial 0057 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 32\n",
            "dropout1: 0.3556756662176866\n",
            "intermediate_dim2: 416\n",
            "dropout2: 0.123821601692504\n",
            "intermediate_dim3: 512\n",
            "dropout3: 0.19732349795784043\n",
            "rate: 0.600225005033242\n",
            "lr: 0.0002580684373370807\n",
            "weight_decay: 0.00015385253510338064\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 4.206549419905059e-05\n",
            "\n",
            "Trial 0036 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 96\n",
            "dropout1: 0.25765239534889905\n",
            "intermediate_dim2: 192\n",
            "dropout2: 0.2394882400562303\n",
            "intermediate_dim3: 96\n",
            "dropout3: 0.25788521145097854\n",
            "rate: 0.36471642757583805\n",
            "lr: 0.001124997459151744\n",
            "weight_decay: 0.0009756992661327715\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 3\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0032\n",
            "Score: 4.20822216256056e-05\n",
            "\n",
            "Trial 0034 summary\n",
            "Hyperparameters:\n",
            "intermediate_dim1: 320\n",
            "dropout1: 0.6645581993871603\n",
            "intermediate_dim2: 32\n",
            "dropout2: 0.2876835368812611\n",
            "intermediate_dim3: 288\n",
            "dropout3: 0.26395242515751427\n",
            "rate: 0.13306342080758968\n",
            "lr: 0.0007098923102025065\n",
            "weight_decay: 0.005306279417939182\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 3\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0005\n",
            "Score: 4.210436236462556e-05\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary()\n",
        "#print(best_hyperparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Weights for model 'sequential_2' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#top_model.save('saved_model2/model_hp.keras')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#top_model.save(\"saved_model2/model_hp3.keras\", save_format=\"h5\", include_optimizer=True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49msave_model(top_model, \u001b[39m'\u001b[39;49m\u001b[39mrnn_models/my_model.keras\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/saving/saving_api.py:145\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     saving_lib\u001b[39m.\u001b[39msave_model(model, filepath)\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39m# Legacy case\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49msave_model(\n\u001b[1;32m    146\u001b[0m         model,\n\u001b[1;32m    147\u001b[0m         filepath,\n\u001b[1;32m    148\u001b[0m         overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    149\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m    150\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    151\u001b[0m     )\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/engine/training.py:3540\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3529\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3531\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3532\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m   3533\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3538\u001b[0m     \u001b[39m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[1;32m   3539\u001b[0m     \u001b[39m# defined.\u001b[39;00m\n\u001b[0;32m-> 3540\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3541\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m have not yet been \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3542\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWeights are created when the model is first called on \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3545\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Weights for model 'sequential_2' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`."
          ]
        }
      ],
      "source": [
        "#top_model.save('saved_model2/model_hp.keras')\n",
        "#top_model.save(\"saved_model2/model_hp3.keras\", save_format=\"h5\", include_optimizer=True)\n",
        "keras.models.save_model(top_model, 'rnn_models/my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_model\u001b[39m.\u001b[39;49msummary()\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/engine/training.py:3229\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m \n\u001b[1;32m   3200\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3226\u001b[0m \u001b[39m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m-> 3229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3232\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3233\u001b[0m     )\n\u001b[1;32m   3234\u001b[0m layer_utils\u001b[39m.\u001b[39mprint_summary(\n\u001b[1;32m   3235\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3236\u001b[0m     line_length\u001b[39m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3241\u001b[0m     layer_range\u001b[39m=\u001b[39mlayer_range,\n\u001b[1;32m   3242\u001b[0m )\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ],
      "source": [
        "top_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = keras.Sequential([\n",
        "    # keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\"),\n",
        "    # keras.Input(shape=(None,), dtype=\"int64\",),\n",
        "    # keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    #     vocabulary_size=VOCAB_SIZE,\n",
        "    #     sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    #     embedding_dim=EMBED_DIM,\n",
        "    #     mask_zero=True,\n",
        "    # ),\n",
        "    #tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "\n",
        "    encoder2,\n",
        "    keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=VOCAB_SIZE,\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        embedding_dim=EMBED_DIM,\n",
        "        mask_zero=True,\n",
        "    ),\n",
        "    # tf.keras.layers.Embedding(len(encoder2.get_vocabulary()),\n",
        "    #                         #input_length = MAX_SEQUENCE_LENGTH,\n",
        "    #                         output_dim=hp.Int(\"output_dim\", min_value=8, max_value=128, step=32), \n",
        "    #                         mask_zero=True),\n",
        "    #tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Reshape((600,128), input_shape=(None,None,600,128)),\n",
        "    #tf.reshape(X, shape=[tf.shape(X)[0]*tf.shape(x)[1],2]),\n",
        "    #keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dropout(rate=hp.Float(\"rate\", 0.1, 0.99, sampling=\"log\")),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=108)),\n",
        "    keras_nlp.layers.FNetEncoder(intermediate_dim=104,\n",
        "                                dropout=0.2888822106618751),\n",
        "    keras_nlp.layers.FNetEncoder(intermediate_dim=72,\n",
        "                                dropout=0.1477764407983245),\n",
        "    keras_nlp.layers.FNetEncoder(intermediate_dim=104,\n",
        "                                dropout=0.1516567561421836),\n",
        "    tf.keras.layers.Dropout(rate=0.5964271961021104),\n",
        "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=hp.Int(\"units\", min_value=4, max_value=128, step=8), return_sequences=hp.Boolean(\"return_sequences\"))),\n",
        "    \n",
        "    tf.keras.layers.Dense(3, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "top_model.compile(\n",
        "    optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(tf.keras.optimizers.AdamW(\n",
        "                    learning_rate=0.0002502739699524452,\n",
        "                    weight_decay=0.007652624915370285)),\n",
        "    loss=\"mean_squared_error\", \n",
        "    metrics=[\n",
        "            #tf.keras.metrics.AUC(name=\"roc\"),\n",
        "            #tf.keras.metrics.AUC(curve='PR', name='pr'),\n",
        "            #keras.metrics.CategoricalCrossentropy(),\n",
        "            #tf.keras.metrics.R2Score(),\n",
        "            r2_score,\n",
        "            tf.keras.metrics.MeanSquaredError(),\n",
        "            tf.keras.metrics.MeanAbsoluteError(),\n",
        "            tf.keras.metrics.MeanAbsolutePercentageError(),\n",
        "            tf.keras.metrics.MeanSquaredLogarithmicError(),\n",
        "            tf.keras.metrics.RootMeanSquaredError(),\n",
        "            #tf.keras.metrics.Accuracy(), \n",
        "            'acc'\n",
        "    ],\n",
        "    jit_compile=True\n",
        ")\n",
        "\n",
        "# lr: 0.0002502739699524452\n",
        "# weight_decay: 0.007652624915370285\n",
        "# best_model.compile(\n",
        "#     optimizer=tf.keras.optimizers.AdamW(0.000539411439833416), \n",
        "#     loss=\"categorical_crossentropy\", \n",
        "#     metrics=[keras.metrics.CategoricalAccuracy(), \n",
        "#             tf.keras.metrics.AUC(name=\"roc\"),\n",
        "#             tf.keras.metrics.AUC(curve='PR', name='pr'),\n",
        "#             keras.metrics.CategoricalCrossentropy()\n",
        "#         ])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_mean_squared_error',\n",
        "    mode='min',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_size = len(X_train)\n",
        "steps_per_epoch = int(train_data_size / 64)\n",
        "#epoches = steps_per_epoch * train_data_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "278/990 [=======>......................] - ETA: 7:08 - loss: 1483091328.0000 - r2_score: -0.2534 - mean_squared_error: 1483091328.0000 - mean_absolute_error: 11875.5322 - mean_absolute_percentage_error: 99.9529 - mean_squared_logarithmic_error: 59.7520 - root_mean_squared_error: 38510.9219 - acc: 0.0000e+00"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history2 \u001b[39m=\u001b[39m top_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m                     full_train_ds,\n\u001b[1;32m      3\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[1;32m      5\u001b[0m                     validation_steps\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                     callbacks \u001b[39m=\u001b[39;49m [callback, tensorboard_callback, model_checkpoint_callback],\n\u001b[1;32m      7\u001b[0m                     \u001b[39m#steps_per_epoch=steps_per_epoch,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m                     )\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history2 = top_model.fit(\n",
        "                    full_train_ds,\n",
        "                    epochs=3,\n",
        "                    validation_data=val_ds,\n",
        "                    validation_steps=30,\n",
        "                    callbacks = [callback, tensorboard_callback, model_checkpoint_callback],\n",
        "                    #steps_per_epoch=steps_per_epoch,\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_model\u001b[39m.\u001b[39;49msummary()\n",
            "File \u001b[0;32m~/Repos/Jira_Oracle/.venv/lib/python3.11/site-packages/keras/engine/training.py:3229\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m \n\u001b[1;32m   3200\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3226\u001b[0m \u001b[39m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m-> 3229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3232\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3233\u001b[0m     )\n\u001b[1;32m   3234\u001b[0m layer_utils\u001b[39m.\u001b[39mprint_summary(\n\u001b[1;32m   3235\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3236\u001b[0m     line_length\u001b[39m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3241\u001b[0m     layer_range\u001b[39m=\u001b[39mlayer_range,\n\u001b[1;32m   3242\u001b[0m )\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ],
      "source": [
        "top_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "275/275 [==============================] - 3s 9ms/step - loss: 1.0987e-04 - mean_squared_error: 1.0987e-04 - mean_absolute_error: 0.0021 - mean_absolute_percentage_error: 175.3331 - mean_squared_logarithmic_error: 8.5368e-05 - root_mean_squared_error: 0.0105 - acc: 5.6825e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.00010986616689478979,\n",
              " 0.00010986616689478979,\n",
              " 0.002054939279332757,\n",
              " 175.33309936523438,\n",
              " 8.536754467058927e-05,\n",
              " 0.010481705889105797,\n",
              " 5.682463961420581e-05]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Unsupported signature for serialization: ((IndexedSlicesSpec(TensorShape([None, 64]), tf.float32, tf.int32, tf.int32, TensorShape([None])), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58ec9610>, 140528528571760), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(256, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58ee7e90>, 140528528571648), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58eecd50>, 140528528606432), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58ecd4d0>, 140528528605472), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58f11bd0>, 140528528608672), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d16510>, 140528528325760), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 512), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d2ecd0>, 140528528325200), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(512,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d3b650>, 140528528323680), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d4bf50>, 140528572907024), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d509d0>, 140528567982704), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d61550>, 140528527213472), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d6e210>, 140528527213872), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d86cd0>, 140528527214992), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d9f890>, 140528527215392), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 128), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dabf50>, 140528527216272), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(128,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58db4b90>, 140528527216672), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dbd410>, 140528527217472), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dcdb90>, 140528527217872), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dda610>, 140528527224432), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58deed90>, 140528527224832), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58e03590>, 140528527225632), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58e13c90>, 140528527226032), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 256), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff0bdd0>, 140528527226832), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(256,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff18dd0>, 140528527227232), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff1d550>, 140528527228032), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff2dbd0>, 140528527228432), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff4a2d0>, 140528527429184), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff62b90>, 140528527433904), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((IndexedSlicesSpec(TensorShape([None, 64]), tf.float32, tf.int32, tf.int32, TensorShape([None])), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58ec9610>, 140528528571760), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(256, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58ee7e90>, 140528528571648), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58eecd50>, 140528528606432), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58ecd4d0>, 140528528605472), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58f11bd0>, 140528528608672), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d16510>, 140528528325760), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 512), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d2ecd0>, 140528528325200), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(512,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d3b650>, 140528528323680), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d4bf50>, 140528572907024), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d509d0>, 140528567982704), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d61550>, 140528527213472), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d6e210>, 140528527213872), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d86cd0>, 140528527214992), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58d9f890>, 140528527215392), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 128), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dabf50>, 140528527216272), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(128,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58db4b90>, 140528527216672), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dbd410>, 140528527217472), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dcdb90>, 140528527217872), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58dda610>, 140528527224432), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58deed90>, 140528527224832), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58e03590>, 140528527225632), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf58e13c90>, 140528527226032), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 256), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff0bdd0>, 140528527226832), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(256,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff18dd0>, 140528527227232), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 64), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff1d550>, 140528527228032), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff2dbd0>, 140528527228432), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(64, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff4a2d0>, 140528527429184), {}).\n",
            "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fcf0ff62b90>, 140528527433904), {}).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, token_embedding4_layer_call_fn, token_embedding4_layer_call_and_return_conditional_losses, position_embedding4_layer_call_fn, position_embedding4_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: rnn_models/my_model.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: rnn_models/my_model.tf/assets\n"
          ]
        }
      ],
      "source": [
        "keras.models.save_model(top_model, 'rnn_models/my_model.tf',\n",
        "                        overwrite= True,\n",
        "                        save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_config = top_model.to_json()\n",
        "top_model.save_weights(\"rnn_models/my_model.weights.h5\")\n",
        "top_model.load_weights(\"rnn_models/my_model.weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_dataset.cardinality().numpy())\n",
        "for element in train_dataset:\n",
        "    print(element)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# model3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tickets = pd.read_csv(\"combined_dataset.csv\", skip_blank_lines = False, keep_default_na=False)\n",
        "tickets = pd.read_csv(\"combined_dataset2.csv\", skip_blank_lines = False, keep_default_na=False)\n",
        "tickets = tickets.fillna('[UNK]')\n",
        "tickets2 = pd.read_csv(\"labelled_dataset.csv\")\n",
        "tickets3 = pd.DataFrame()\n",
        "\n",
        "#print(tickets['desc'][:3])\n",
        "#tickets['desc'] = [parse_text(text) for text in tickets['desc']]\n",
        "#tickets['desc'] = tickets['desc'].apply(parse_text())\n",
        "#print(tickets['desc'][:3])\n",
        "\n",
        "text_column = tickets['desc']\n",
        "# tickets3 = pd.concat([tickets3,text_column], axis = 1)\n",
        "label_column = tickets2['Grade'].astype(int)\n",
        "label_column = label_column - 1\n",
        "# tickets3 = pd.concat([tickets3,label_column], axis = 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_column, label_column, test_size=0.2, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True)\n",
        "\n",
        "# y_train_cat = to_categorical(y_train, 3)\n",
        "# y_test_cat = to_categorical(y_test, 3)\n",
        "# y_val_cat = to_categorical(y_val, 3)\n",
        "\n",
        "\n",
        "# X_train_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "# X_test_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "X_val_dataset = tf.data.Dataset.from_tensor_slices(X_val)\n",
        "# # y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "# # y_test_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "# y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train_cat)\n",
        "# y_test_dataset = tf.data.Dataset.from_tensor_slices(y_test_cat)\n",
        "y_val_dataset = tf.data.Dataset.from_tensor_slices(y_val)\n",
        "\n",
        "# test_dataset = tf.data.Dataset.zip((X_test_dataset, y_test_dataset))\n",
        "# train_dataset = tf.data.Dataset.zip((X_train_dataset, y_train_dataset))\n",
        "val_dataset = tf.data.Dataset.zip((X_val_dataset, y_val_dataset))\n",
        "\n",
        "features = list(X_train)\n",
        "labels = list(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_column[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samp0 = 54046\n",
        "samp1 = 24077\n",
        "samp2 = 9863\n",
        "tot_samples = samp0 + samp1 + samp2\n",
        "\n",
        "w0 = tot_samples / (3*samp0)\n",
        "w1 = tot_samples / (3*samp1)\n",
        "w2 = tot_samples / (3*samp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
        "    \"deberta_v3_base_en\"\n",
        ")\n",
        "\n",
        "text_list = list(text_column)\n",
        "print(text_list[:5])\n",
        "# Tokenize a batch of single sentences.\n",
        "preprocessor(text_list)\n",
        "\n",
        "\n",
        "\n",
        "# # Custom vocabulary.\n",
        "# bytes_io = io.BytesIO()\n",
        "# ds = tf.data.Dataset.from_tensor_slices(text_list)\n",
        "# sentencepiece.SentencePieceTrainer.train(\n",
        "#     sentence_iterator=ds.as_numpy_iterator(),\n",
        "#     model_writer=bytes_io,\n",
        "#     vocab_size=25994,\n",
        "#     model_type=\"WORD\",\n",
        "#     pad_id=0,\n",
        "#     bos_id=1,\n",
        "#     eos_id=2,\n",
        "#     unk_id=3,\n",
        "#     pad_piece=\"[PAD]\",\n",
        "#     bos_piece=\"[CLS]\",\n",
        "#     eos_piece=\"[SEP]\",\n",
        "#     unk_piece=\"[UNK]\",\n",
        "# )\n",
        "# tokenizer = keras_nlp.models.DebertaV3Tokenizer(\n",
        "#     proto=bytes_io.getvalue(),\n",
        "# )\n",
        "# preprocessor = keras_nlp.models.DebertaV3Preprocessor(tokenizer)\n",
        "# preprocessor(text_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "\n",
        "\n",
        "bytes_io = io.BytesIO()\n",
        "\n",
        "# tokenizer = keras_nlp.models.DebertaV3Tokenizer.from_preset(\n",
        "#     \"deberta_v3_base_en\",\n",
        "#     #proto=bytes_io.getvalue(),\n",
        "# )\n",
        "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
        "    \"deberta_v3_base_en\",\n",
        "    #tokenizer=tokenizer,\n",
        "    sequence_length=128,\n",
        ")\n",
        "\n",
        "# Pretrained classifier.\n",
        "classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n",
        "    \"deberta_v3_base_en\",\n",
        "    num_classes=3,\n",
        "    dropout = 0.2,\n",
        "    #hidden_dim = 2,\n",
        "    preprocessor = preprocessor,\n",
        "    activation = \"softmax\",\n",
        "    \n",
        ")\n",
        "\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=keras.optimizers.Adam(5e-5),\n",
        "    jit_compile=True,\n",
        "\n",
        "    metrics=['accuracy', \n",
        "        #keras.metrics.SparseCategoricalCrossentropy(),\n",
        "        'crossentropy',\n",
        "        #tf.keras.metrics.AUC(name=\"roc\"),\n",
        "        #tf.keras.metrics.AUC(curve='PR', name='pr'),\n",
        "    ],\n",
        "    ) \n",
        "\n",
        "# classifier.fit(x=features, y=labels, batch_size=64)\n",
        "#classifier.predict(x=features, batch_size=64)\n",
        "\n",
        "classifier.fit(x=features,\n",
        "                y=labels,\n",
        "                batch_size=64,\n",
        "                class_weight = {0:w0,1:w1,2:w2},\n",
        "                validation_data=val_dataset\n",
        "                )\n",
        "\n",
        "# # Re-compile (e.g., with a new learning rate).\n",
        "# classifier.compile(\n",
        "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#     optimizer=keras.optimizers.Adam(5e-5),\n",
        "#     jit_compile=True,\n",
        "# )\n",
        "# # Access backbone programatically (e.g., to change `trainable`).\n",
        "# classifier.backbone.trainable = False\n",
        "# # Fit again.\n",
        "# classifier.fit(x=features, y=labels, batch_size=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classifier.get_config())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = classifier.evaluate(X_test,y_test)\n",
        "for name, value in zip(classifier.metrics_names, results):\n",
        "    print(name, ': ', value)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_rnn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
