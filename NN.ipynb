{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "## Load in data\n",
    "\n",
    "tickets = pd.read_csv(\"master_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bagOfWordsEncoder(vocab, input):\n",
    "    ## Make an array size of the vocab\n",
    "    encoderMatrix = np.zeros(len(vocab))\n",
    "\n",
    "    ## Iterate the input strings and encode their position in the array\n",
    "    for word in input:\n",
    "        index = vocab[vocab['word']==word].index.values\n",
    "        encoderMatrix[index] += 1\n",
    "    \n",
    "    if encoderMatrix.std() != 0:\n",
    "        encoderMatrix = (encoderMatrix - encoderMatrix.mean()) / encoderMatrix.std()\n",
    "    return encoderMatrix\n",
    "\n",
    "## Method for getting input\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def parse_text(text):\n",
    "\n",
    "    ## Tokenize string into words (and punctuation)\n",
    "    word_array = word_tokenize(text)\n",
    "    word_array = [word.lower() for word in word_array if word.isalpha()]\n",
    "\n",
    "    ## Filter out stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in word_array if word.casefold() not in stop_words]\n",
    "\n",
    "    ## Turn words into lemmatized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemitized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "    ## Apply Stemming (Find the roots of similar words)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in lemitized_words]\n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = pd.read_csv(\"reduced_vocabulary.csv\")\n",
    "#vocab = vocab['word'].tolist()\n",
    "encodings = []\n",
    "efforts = tickets['effort(s)'].to_numpy()\n",
    "\n",
    "## Count tickets\n",
    "\n",
    "descs = tickets['desc'].to_numpy()\n",
    "for desc in descs:\n",
    "    parsed = parse_text(desc)\n",
    "    encoding = bagOfWordsEncoder(vocab, parsed)\n",
    "    encodings.append(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000\n",
      "19000\n",
      "19000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Gather training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "labels = pd.read_csv(\"labelled_dataset.csv\")\n",
    "\n",
    "labels = labels['Grade'].tolist()\n",
    "labels = [(label - 1)/2 for label in labels]\n",
    "\n",
    "data = {'encoding': encodings, 'effort': labels}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "## simplify data ~ 10000\n",
    "df_low_effort = df[df.effort == 0.0].sample(19000)\n",
    "df_med_effort = df[df.effort == 0.5].sample(19000)\n",
    "df_hig_effort = df[df.effort == 1.0].sample(19000)\n",
    "\n",
    "print(len(df_low_effort))\n",
    "print(len(df_med_effort))\n",
    "print(len(df_hig_effort))\n",
    "\n",
    "result = pd.concat([df_low_effort, df_med_effort, df_hig_effort], axis=0)\n",
    "\n",
    "train, test = train_test_split(result, test_size=0.2, shuffle=True)\n",
    "\n",
    "x_train = tf.convert_to_tensor(train['encoding'].to_list())\n",
    "y_train = tf.convert_to_tensor(train['effort'])\n",
    "\n",
    "x_test = tf.convert_to_tensor(test['encoding'].to_list())\n",
    "y_test = tf.convert_to_tensor(test['effort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1425/1425 [==============================] - 13s 8ms/step - loss: 0.1617 - accuracy: 0.4258\n",
      "Epoch 2/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.1266 - accuracy: 0.5005\n",
      "Epoch 3/20\n",
      "1425/1425 [==============================] - 16s 11ms/step - loss: 0.0989 - accuracy: 0.5606\n",
      "Epoch 4/20\n",
      "1425/1425 [==============================] - 15s 11ms/step - loss: 0.0664 - accuracy: 0.6153\n",
      "Epoch 5/20\n",
      "1425/1425 [==============================] - 16s 11ms/step - loss: 0.0462 - accuracy: 0.6397\n",
      "Epoch 6/20\n",
      "1425/1425 [==============================] - 16s 11ms/step - loss: 0.0351 - accuracy: 0.6503\n",
      "Epoch 7/20\n",
      "1425/1425 [==============================] - 15s 10ms/step - loss: 0.0305 - accuracy: 0.6542\n",
      "Epoch 8/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.0265 - accuracy: 0.6558\n",
      "Epoch 9/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.0235 - accuracy: 0.6562\n",
      "Epoch 10/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.0210 - accuracy: 0.6571\n",
      "Epoch 11/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.0191 - accuracy: 0.6574\n",
      "Epoch 12/20\n",
      "1425/1425 [==============================] - 15s 10ms/step - loss: 0.0173 - accuracy: 0.6580\n",
      "Epoch 13/20\n",
      "1425/1425 [==============================] - 15s 10ms/step - loss: 0.0162 - accuracy: 0.6581\n",
      "Epoch 14/20\n",
      "1425/1425 [==============================] - 15s 11ms/step - loss: 0.0152 - accuracy: 0.6583\n",
      "Epoch 15/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.0143 - accuracy: 0.6587\n",
      "Epoch 16/20\n",
      "1425/1425 [==============================] - 13s 9ms/step - loss: 0.0141 - accuracy: 0.6586\n",
      "Epoch 17/20\n",
      "1425/1425 [==============================] - 13s 9ms/step - loss: 0.0133 - accuracy: 0.6591\n",
      "Epoch 18/20\n",
      "1425/1425 [==============================] - 13s 9ms/step - loss: 0.0122 - accuracy: 0.6584\n",
      "Epoch 19/20\n",
      "1425/1425 [==============================] - 14s 10ms/step - loss: 0.0117 - accuracy: 0.6590\n",
      "Epoch 20/20\n",
      "1425/1425 [==============================] - 13s 9ms/step - loss: 0.0113 - accuracy: 0.6591\n",
      "357/357 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.4405\n",
      "Tested Acc:  0.4405263066291809\n"
     ]
    }
   ],
   "source": [
    "## SETUP NEURAL NETWORK\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(2742,1)),\n",
    "    \n",
    "    keras.layers.Dense(256, activation=\"relu\"),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(256, activation=\"relu\"),\n",
    "    #keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.1), loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Tested Acc: \", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
